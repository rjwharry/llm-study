{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @chain Decorator\n",
    "`RunnableLambda`ì™€ ê¸°ëŠ¥ì ìœ¼ë¡œ ë™ì¼í•˜ë©°, ì„ì˜ì˜ í•¨ìˆ˜ì— `@chain` decoratorë¥¼ ì¶”ê°€í•˜ì—¬ chainìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API í‚¤ ì •ë³´ ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{topic} ì— ëŒ€í•´ ì§§ê²Œ í•œê¸€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{sentence} ë¥¼ emojië¥¼ í™œìš©í•œ ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œê¸€ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def custom_chain(text):\n",
    "    chain1 = prompt1 | ChatOllama(model=\"gemma:7b\") | StrOutputParser()\n",
    "    output1 = chain1.invoke({\"topic\": text})\n",
    "\n",
    "    chain2 = prompt2 | ChatOllama(model=\"mistral:7b\") | StrOutputParser()\n",
    "    return chain2.invoke({\"sentence\": output1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ğŸ‰Large Language Model (LLM) ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì˜ë¯¸í•©ë‹ˆë‹¤! ğŸ¤– AI ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ì´í•´í•˜ê³  ìƒì„±í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ğŸ“š ê´‘ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘í•œ ìƒí™©ì— ë§ì¶° ë¬¸ì¥ì„ ìƒì„±í•˜ê³  ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ˜ŠğŸ‘'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_chain.invoke(\"LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
