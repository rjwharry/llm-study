{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallback\n",
    "- LLM 애플리케이션을 개발/운영하다보면 품질 저하, API 오류 등의 문제가 발생하는데, 에러 처리를 gracefully하게 하고 격리하는데 fallback을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM API\n",
    "- 많은 경우에 LLM API 오류에 대해서 Fallback을 많이 사용한다.\n",
    "- API 오류는 비용 초과, 속도 제한 등의 다양한 오류가 종종 발생하므로, LLM 애플리케이션을 안정성있게 만들 수 있다\n",
    "- `with_fallbacks()` 함수를 사용하여 만약 llm api 실패했을 때 함수의 매개변수로 넣은 다른 모델로 재시도한다.\n",
    "- `with_fallbacks`에서 `exceptions_to_handle` 매개변수에 exception을 넣으면, 해당 exception이 발생했을 때만 fallback을 실행시켜서 불필요한 fallback 호출을 줄이고, 에러처리를 더 세밀하게 다룰 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatOllama Fallback\n",
    "ChatOllama도 `with_fallbacks` 함수를 사용할 수 있으며, 첫 번째 예제에서는 모델 이름을 잘못 입력해서 에러가 발생했지만, 두 번째에서는 `with_fallbacks` 덕분에 에러나지 않고 mistral 모델로 추론하는 모습을 볼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull gemma:7t`.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize primary ChatOllama model\n",
    "llm = ChatOllama(model=\"gemma:7t\")\n",
    "\n",
    "# Create a simple prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic} 에 대하여 간략히 설명해 줘.\")\n",
    "\n",
    "# Create a chain using LCEL\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "try:\n",
    "    result = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial Intelligence (AI)는 인공 생명을 모방하거나 이해하는 데 사용되는 소프트웨어 및 하드웨어의 집합입니다. AI는 기계가 학습, 퍼цеп션, 추론 등 인간과 유사한 능력을 발전시키고 이용할 수 있도록 설계되었습니다. AI는 크게 3가지로 구분됩니다:\n",
      "\n",
      "1. Weak AI (Narrow AI): AI 모델은 특정 작업에만 적용되며, 인간이 수행하는 일반적인 활동보다 한정된 영역에서의 지능을 시뮬레이션합니다.\n",
      "\n",
      "2. General AI (AGI): AI 모델은 인간과 유사한 수준의 능력으로, 어떤 작업도 학습하고 수행할 수 있습니다.\n",
      "\n",
      "3. Superintelligent AI (ASI): AI 모델은 인간이나 그 이상의 수준의 지능을 갖습니다. ASI는 현재 기술력으로는 구현되기 어렵거나 불가능합니다.\n",
      "\n",
      "AI는 인공 신경망(Neural Networks), 머신러닝, 데이터 분석, 로봇 투영기, 자율주행 차량 등 잠재력이 巨大한 분야에서 활용되고 있으며, 지속적인 발전을 거듭니다.\n"
     ]
    }
   ],
   "source": [
    "# Initialize fallback ChatOllama model (using a different model)\n",
    "fallback_llm = ChatOllama(model=\"mistral:7b\")\n",
    "\n",
    "# Combine with fallback\n",
    "llm = llm.with_fallbacks([fallback_llm])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "try:\n",
    "    result = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
