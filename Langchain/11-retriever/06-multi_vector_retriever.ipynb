{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Scikit Learn\\n\\nScikit-learn은 Python 언어를 위한 또 다른 핵심 라이브러리로, 기계 학습의 다양한 알고리즘을 구현하기 위해 설계되었습니다. 이 라이브러리는 2007년 David Cournapeau에 의해 프로젝트가 시작되었으며, 그 후로 커뮤니티의 광범위한 기여를 받아 현재까지 발전해왔습니다. Scikit-learn은 분류, 회귀, 군집화, 차원 축소 등 다양한 기계 학습 작업을 지원하며, 사용하기 쉬운 API와 함께 제공되어 연구자와 개발자가 복잡한 데이터 과학 문제를 해결할 수 있도록 돕습니다.\\n\\n핵심 특징 중 하나는 다양한 기계 학습 모델을 일관된 인터페이스로 제공한다는 점입니다. 이는 사용자가 알고리즘 간에 쉽게 전환할 수 있게 하여, 최적의 모델을 찾는 과정을 단순화합니다. 또한, Scikit-learn은 사전 처리, 모델 선택, 평가 지표 등 기계 학습 파이프라인의 다른 단계에 필요한 도구들을 포함하고 있습니다. 이는 연구자와 개발자가 데이터를 더 효율적으로 처리하고, 모델의 성능을 정확히 평가할 수 있게 해줍니다.\\n\\nScikit-learn의 강점은 그의 범용성에 있습니다. 이는 다양한 산업과 연구 분야에서 사용될 수 있으며, 소규모 데이터셋부터 대규모 데이터셋까지 다양한 크기의 데이터를 처리할 수 있습니다. 또한, 오픈 소스 프로젝트로서, Scikit-learn은 지속적인 개선과 업데이트가 이루어지며, 사용자 커뮤니티로부터의 피드백과 기여를 받아 발전해 나갑니다.\\n\\n기계 학습 분야에 있어서, Scikit-learn은 특히 초보자에게 친화적인 학습 자원을 제공함으로써, 복잡한 이론과 알고리즘을 이해하는 데 중요한 역할을 합니다. 이러한 접근성과 범용성은 Scikit-learn을 기계 학습을 시작하는 이들에게 인기 있는 선택지로 만들었습니다.\\n\\nNLP\\n\\nNLP(자연어 처리)는 인간의 언어를 이해하고 해석하는 컴퓨터 알고리즘과 기술의 집합입니다. 이 분야는 컴퓨터 과학, 인공 지능, 언어학의 교차점에 위치하며, 텍스트와 음성 데이터를 처리하여 사람과 컴퓨터 간의 상호작용을 보다 자연스럽고 효율적으로 만드는 것을 목표로 합니다. NLP의 주요 응용 분야로는 기계 번역, 감정 분석, 음성 인식, 자동 요약, 챗봇 개발 등이 있으며, 이러한 기술은 정보 검색, 교육, 의료, 고객 서비스 등 다양한 분야에서 활용됩니다.\\n\\nNLP의 핵심 과제 중 하나는 자연어의 모호성과 다양성을 처리하는 것입니다. 인간 언어는 복잡하고, 문맥에 따라 그 의미가 달라질 수 있으며, 같은 단어나 구문이 다양한 의미로 사용될 수 있습니다. 이러한 언어의 특성으로 인해, NLP 시스템은 텍스트의 문법적 구조를 파악하고, 문맥을 분석하여 의미를 정확하게 이해할 수 있어야 합니다. 이를 위해 NLP는 통계학, 기계 학습, 딥 러닝과 같은 다양한 수학적 및 계산적 방법론을 활용합니다.\\n\\n최근 몇 년 동안, 딥 러닝 기반의 NLP 모델, 특히 변환기(Transformer) 아키텍처와 같은 신경망 모델의 발전으로 인해, 자연어 이해와 생성 분야에서 놀라운 진보가 이루어졌습니다. 이러한 모델들은 대규모의 언어 데이터에서 학습하여, 문장의 문맥을 효과적으로 파악하고, 보다 정교한 언어 이해 및 생성 능력을 제공합니다.\\n\\nNLP의 발전은 사람과 컴퓨터 간의 소통 방식을 근본적으로 변화시켰습니다. 예를 들어, 자연어를 이해할 수 있는 인터페이스를 통해 사용자는 복잡한 쿼리나 명령어를 사용하지 않고도 정보를 검색하거나 서비스를 이용할 수 있게 되었습니다. 또한, 자동 번역 시스템의 발전으로 언어 장벽이 낮아지고 있으며, 감정 분석 기술을 통해 소셜 미디어나 고객 리뷰에서 유용한 인사이트를 추출할 수 있게 되었습니다.\\n\\nNLP는 계속해서 발전하고 있으며, 이 분야의 연구와 기술 혁신은 인간 언어의 복잡성을 더 깊이 이해하고, 인간과 기계 간의 상호작용을 더욱 풍부하고 의미 있게 만드는 데 기여할 것입니다.\\n\\nScipy\\n\\nSciPy는 과학 계산을 위한 중심적인 파이썬 라이브러리 중 하나로, 고급 수학 함수, 알고리즘, 그리고 편리한 도구를 제공합니다. 이 라이브러리는 2001년에 Travis Oliphant에 의해 처음 개발되었으며, 그 이후로 수학, 과학, 엔지니어링 커뮤니티에 광범위하게 사용되고 있습니다. SciPy는 NumPy 배열 객체 위에 구축되어 있으며, NumPy의 기능을 확장하여 다양한 과학 및 공학 분야에서 필요로 하는 특정한 기능과 유틸리티를 제공합니다.\\n\\nSciPy의 주요 모듈에는 최적화, 선형 대수, 적분, 보간, 특수 함수, 신호 처리, 이미지 처리 등이 포함됩니다. 이러한 모듈들은 고급 수학적 연산과 과학적 분석을 수행할 수 있는 강력한 도구를 제공합니다. 예를 들어, 최적화 모듈을 사용하면 함수의 최소값을 찾거나, 방정식 시스템의 해를 구할 수 있습니다. 선형 대수 모듈을 통해서는 행렬 분해, 고유값 분석, 선형 시스템 해법 등을 계산할 수 있습니다.\\n\\nSciPy 라이브러리는 연구자와 엔지니어가 복잡한 수학적 문제를 효율적으로 해결할 수 있도록 설계되었습니다. 그 사용법은 상대적으로 간단하며, Python의 다른 과학 기술 스택과 함께 사용될 때 그 잠재력이 극대화됩니다. 예를 들어, Matplotlib과 결합하여 과학적 데이터를 시각화할 수 있고, Pandas와 함께 데이터 분석을 수행할 수 있습니다.\\n\\nSciPy의 개발은 오픈 소스 커뮤니티의 기여를 통해 이루어지며, 지속적인 개선과 업데이트가 진행되고 있습니다. 이는 라이브러리가 최신 과학적 방법론과 계산 기술을 반영하도록 보장합니다. SciPy의 사용과 배포는 BSD 라이선스 하에 이루어져, 학술 연구는 물론 상업적 응용 프로그램에도 자유롭게 사용될 수 있습니다.\\n\\n종합적으로, SciPy는 과학 계산에 필요한 광범위한 기능을 제공하며, 파이썬을 과학, 엔지니어링, 수학 분야의 연구와 개발에 적합한 언어로 만드는 데 핵심적인 역할을 합니다.\\n\\nHuggingFace\\n\\nHugging Face는 자연어 처리(NLP) 분야에서 선도적인 역할을 하는 기술 회사로, 인공 지능(AI) 연구와 응용 프로그램 개발을 위한 다양한 오픈 소스 도구와 라이브러리를 제공합니다. 2016년에 설립된 이 회사는 특히 \\'Transformers\\' 라이브러리를 통해 큰 인기를 얻었습니다. Transformers 라이브러리는 BERT, GPT, RoBERTa, T5와 같은 최신의 변환기(Transformer) 기반 모델을 쉽게 사용할 수 있게 해주며, Python 프로그래밍 언어로 작성되어 있습니다. 이 라이브러리의 주요 목적은 최신의 NLP 기술을 쉽고 효율적으로 접근할 수 있도록 하는 것입니다.\\n\\nHugging Face의 Transformers 라이브러리는 다양한 NLP 작업에 활용될 수 있습니다. 이에는 텍스트 분류, 정보 추출, 질문 응답, 요약 생성, 텍스트 생성 등이 포함되며, 다양한 언어에 대한 지원도 제공합니다. 라이브러리는 사전 훈련된 모델을 포함하고 있어, 사용자가 복잡한 모델을 처음부터 훈련시키지 않고도, 빠르게 고성능의 NLP 시스템을 구축할 수 있게 합니다.\\n\\nHugging Face는 또한 \\'Model Hub\\'를 운영하고 있습니다. 이는 수천 개의 사전 훈련된 모델과 데이터셋을 호스팅하는 플랫폼으로, 연구자와 개발자가 자신의 모델을 공유하고 다른 사람들의 모델을 쉽게 찾아 사용할 수 있게 합니다. Model Hub를 통해 사용자는 특정 NLP 작업에 최적화된 모델을 검색하고, 직접 훈련시킨 모델을 커뮤니티와 공유할 수 있습니다.\\n\\nHugging Face는 또한 AI 연구 커뮤니티에 대한 기여를 넘어, 윤리적 AI 개발과 모델의 공정성 및 투명성에 대한 중요성을 강조합니다. 회사는 AI 기술의 사회적 영향에 대해 적극적으로 논의하고, 모델 배포 시 발생할 수 있는 윤리적 문제를 해결하기 위한 가이드라인을 제공하려 노력합니다.\\n\\n종합적으로 볼 때, Hugging Face는 NLP 및 AI 분야에서 중요한 자원을 제공하는 회사로, 오픈 소스 기여와 커뮤니티 구축을 통해 AI 기술의 발전과 보급에 기여하고 있습니다. 이를 통해 연구자, 개발자, 기업 등 다양한 사용자가 최신 AI 기술을 쉽게 접근하고 활용할 수 있는 환경을 조성하고 있습니다.\\n\\nAttention is all you need\\n\\n\"Attention Is All You Need\"는 2017년에 발표된 논문으로, 변환기(Transformer) 모델의 아키텍처를 처음으로 소개한 연구입니다. Ashish Vaswani와 그의 공동 연구자들에 의해 작성된 이 논문은 자연어 처리(NLP)와 기계 학습 분야에 큰 영향을 미쳤습니다. 변환기 모델은 이전의 순환 신경망(RNN)이나 합성곱 신경망(CNN)에 의존하던 방식에서 벗어나, 전적으로 \\'어텐션 메커니즘(Attention Mechanism)\\'을 사용하여 정보를 처리합니다. 이로 인해 모델의 학습 속도와 효율성이 크게 향상되었습니다.\\n\\n어텐션 메커니즘은 입력 데이터의 서로 다른 부분에 대해 모델이 얼마나 많은 \\'주의\\'를 기울여야 하는지를 결정합니다. 특히, 변환기 모델에서 사용되는 \\'셀프 어텐션(Self-Attention)\\'은 입력 데이터 내의 각 요소가 서로 얼마나 관련이 있는지를 계산하여, 모델이 문맥을 더 잘 이해할 수 있게 합니다. 이는 특히 문장 내에서 단어 간의 관계를 파악하는 데 매우 효과적이며, 문장의 길이에 관계없이 일정한 연산 시간으로 처리할 수 있습니다.\\n\\n\"Attention Is All You Need\" 논문은 변환기 아키텍처를 기반으로 한 \\'멀티헤드 어텐션(Multi-Head Attention)\\' 기법을 도입했습니다. 이는 여러 개의 어텐션 메커니즘을 병렬로 사용하여, 서로 다른 표현 공간에서 정보를 동시에 처리함으로써 모델의 성능을 향상시킵니다. 또한, 논문은 전통적인 RNN이나 CNN에 의존하지 않음으로써 발생하는 병렬 처리의 이점을 강조하며, 이를 통해 훨씬 더 빠른 학습 속도와 효율적인 메모리 사용을 실현합니다.\\n\\n변환기 모델의 이러한 혁신적인 접근 방식은 NLP를 비롯한 여러 분야에서 큰 변화를 가져왔습니다. BERT, GPT, RoBERTa 등의 후속 모델들은 모두 \"Attention Is All You Need\" 논문에서 제시된 변환기 아키텍처를 기반으로 하며, 이들 모델은 텍스트 분류, 요약, 번역, 질문 응답 시스템 등 다양한 작업에서 인상적인 성능을 보여줍니다.\\n\\n이 논문은 기계 학습 및 NLP 분야에서 기존의 접근 방식을 재고하고, 어텐션 메커니즘의 중요성을 강조함으로써 연구와 개발의 새로운 방향을 제시했습니다. \"Attention Is All You Need\"는 그 자체로 혁신적인 연구 성과일 뿐만 아니라, AI 기술의 발전에 있어 중대한 이정표로 평가받고 있습니다.\\n\\nARIMA (자기회귀 누적 이동평균), SARIMA (계절성 자기회귀 누적 이동평균), 그리고 SARIMAX (계절성 자기회귀 누적 이동평균 외부 변수)는 시계열 데이터 분석과 예측을 위한 통계 모델입니다. 이들은 시계열 데이터의 패턴을 이해하고 미래 값을 예측하는 데 널리 사용되며, 경제학, 금융, 기상학 등 다양한 분야에서 응용됩니다. 각 모델의 특징과 차이점을 교수처럼 전문적인 어조로 설명하겠습니다.\\n\\nARIMA (자기회귀 누적 이동평균)\\n\\nARIMA 모델은 비계절적 시계열 데이터를 분석하고 예측하기 위해 설계되었습니다. 이 모델은 세 가지 주요 구성 요소를 기반으로 합니다: 자기회귀(AR) 부분, 차분(I) 부분, 그리고 이동평균(MA) 부분. AR 부분은 이전 관측값의 영향을 모델링하며, I 부분은 데이터를 안정화하는 데 필요한 차분의 횟수를 나타냅니다. MA 부분은 이전 예측 오차의 영향을 모델링합니다. ARIMA 모델은 비계절적 변동성을 가진 데이터에서 유용하게 사용됩니다.\\n\\nSARIMA (계절성 자기회귀 누적 이동평균)\\n\\nSARIMA 모델은 ARIMA 모델을 확장한 것으로, 계절적 변동성을 포함한 시계열 데이터를 분석하고 예측하는 데 적합합니다. SARIMA는 추가적으로 계절성 자기회귀(SAR) 부분, 계절성 차분의 정도, 그리고 계절성 이동평균(SMA) 부분을 모델에 포함시킵니다. 이를 통해 모델은 비계절적 패턴뿐만 아니라 계절적 변동성까지 포괄적으로 고려할 수 있습니다. SARIMA 모델은 주기적인 패턴을 보이는 데이터에 특히 유용합니다.\\n\\nSARIMAX (계절성 자기회귀 누적 이동평균 외부 변수)\\n\\nSARIMAX 모델은 SARIMA에 외부 설명 변수(또는 외생 변수)를 포함시킬 수 있는 확장된 형태입니다. 이 외부 변수들은 시계열 데이터에 영향을 미칠 수 있는 추가적인 정보(예: 경제 지표, 날씨 조건 등)를 모델에 제공합니다. SARIMAX 모델은 이러한 외부 변수들의 영향을 고려함으로써, 예측의 정확도를 더욱 향상시킬 수 있습니다. 이 모델은 복잡한 시계열 데이터에서 다양한 요인들의 영향을 모델링하고자 할 때 유용하게 사용됩니다.\\n\\n각 모델은 특정 유형의 시계열 데이터에 대한 분석과 예측에 있어 강점을 가지며, 선택하는 모델은 분석하고자 하는 데이터의 특성과 요구사항에 따라 달라집니다. ARIMA는 기본적인 비계절적 시계열 분석에 적합하고\\n\\n, SARIMA는 계절적 패턴을 고려해야 하는 경우에 사용됩니다. SARIMAX는 외부 요인을 모델에 포함시켜야 할 때 가장 적합한 선택입니다.\\n\\nWord2Vec\\n\\nWord2Vec은 자연어 처리(NLP) 분야에서 널리 사용되는 획기적인 단어 임베딩 기법 중 하나입니다. 2013년 Google의 연구팀에 의해 개발되었으며, 단어를 벡터 공간에 매핑함으로써 컴퓨터가 단어의 의미를 수치적으로 이해할 수 있게 합니다. Word2Vec의 핵심 아이디어는 비슷한 맥락에서 사용되는 단어들은 비슷한 벡터를 가진다는 것입니다. 이를 통해 단어 간의 의미적 유사성과 관계를 벡터 연산을 통해 표현할 수 있습니다.\\n\\nWord2Vec은 크게 두 가지 모델 아키텍처로 구성됩니다: Continuous Bag-of-Words (CBOW)와 Skip-Gram입니다. CBOW 모델은 주변 단어(맥락)를 기반으로 특정 단어를 예측하는 반면, Skip-Gram 모델은 하나의 단어로부터 주변 단어들을 예측합니다. 두 모델 모두 딥러닝이 아닌, 단순화된 신경망 구조를 사용하여 대규모 텍스트 데이터에서 학습할 수 있으며, 매우 효율적입니다.\\n\\nWord2Vec의 벡터 표현은 다양한 NLP 작업에 활용될 수 있습니다. 예를 들어, 단어의 유사도 측정, 문장이나 문서의 벡터 표현 생성, 기계 번역, 감정 분석 등이 있습니다. 또한, 벡터 연산을 통해 단어 간의 의미적 관계를 추론하는 것이 가능해집니다. 예를 들어, \"king\" - \"man\" + \"woman\"과 같은 벡터 연산을 수행하면, 결과적으로 \"queen\"과 유사한 벡터를 가진 단어를 찾을 수 있습니다.\\n\\nWord2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'source': './data/ai-story.txt'}),\n",
       " Document(page_content='Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken\\n\\n정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\\n예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nTokenizer\\n\\n정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\\n예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nVectorStore\\n\\n정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\\n예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\\n연관키워드: 임베딩, 데이터베이스, 벡터화\\n\\nSQL\\n\\n정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\\n예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\\n연관키워드: 데이터베이스, 쿼리, 데이터 관리\\n\\nCSV\\n\\n정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\\n예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환\\n\\nJSON\\n\\n정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer\\n\\n정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\\n예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention\\n\\nHuggingFace\\n\\n정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\\n예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\\n연관키워드: 자연어 처리, 딥러닝, 라이브러리\\n\\nDigital Transformation\\n\\n정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\\n예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\\n연관키워드: 혁신, 기술, 비즈니스 모델\\n\\nCrawling\\n\\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\\n\\nWord2Vec\\n\\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\\nLLM (Large Language Model)\\n\\n정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\\n예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n연관키워드: 자연어 처리, 딥러닝, 텍스트 생성\\n\\nFAISS (Facebook AI Similarity Search)\\n\\n정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\\n예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\\n\\nOpen Source\\n\\n정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)\\n\\n정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame\\n\\n정의: DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.\\n예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.\\n연관키워드: 데이터 분석, 판다스, 데이터 처리\\n\\nAttention 메커니즘\\n\\n정의: Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 \\'주의\\'를 기울이도록 하는 기법입니다. 이는 주로 시퀀스 데이터(예: 텍스트, 시계열 데이터)에서 사용됩니다.\\n예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\\n연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링\\n\\n판다스 (Pandas)\\n\\n정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.\\n예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리\\n\\nGPT (Generative Pretrained Transformer)\\n\\n정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수 있습니다.\\n예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝\\n\\nInstructGPT\\n\\n정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록 설계되었습니다.\\n예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.\\n연관키워드: 인공지능, 자연어 이해, 명령 기반 처리\\n\\nKeyword Search\\n\\n정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\\n예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색\\n\\nPage Rank\\n\\n정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\\n예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\\n\\n데이터 마이닝\\n\\n정의: 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다. 이는 통계, 머신러닝, 패턴 인식 등의 기술을 활용합니다.\\n예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\\n연관키워드: 빅데이터, 패턴 인식, 예측 분석\\n\\n멀티모달 (Multimodal)\\n\\n정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고 정확한 정보를 추출하거나 예측하는 데 사용됩니다.\\n예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\\n연관키워드: 데이터 융합, 인공지능, 딥러닝', metadata={'source': './data/appendix-keywords.txt'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders = [\n",
    "    TextLoader(\"./data/ai-story.txt\"),\n",
    "    TextLoader(\"./data/appendix-keywords.txt\"),\n",
    "]\n",
    "docs = [loader.load()[0] for loader in loaders]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작은 청크 생성\n",
    "- 대용량 정보를 검색할 때 더 작은 청크로 나눠서 임베딩하는 것이 유용함\n",
    "- 이를 통해 의미론적으로 최대한 근접하게 포착하면서, 가능한 많은 맥락을 하위 단계로 전달할 수 있다\n",
    "- `ParentDocumentRetriever`가 이 작업을 수행한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be9ed566-69ff-469f-8c86-7656ec2692a7',\n",
       " 'be9ed566-69ff-469f-8c86-7656ec2692a7']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=HuggingFaceBgeEmbeddings(),\n",
    ")\n",
    "store = InMemoryByteStore()\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4())] * len(docs)\n",
    "doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000)\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_docs = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    parent_doc = parent_text_splitter.split_documents([doc])\n",
    "    for _doc in parent_doc:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    parent_docs.extend(parent_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    child_doc = child_text_splitter.split_documents([doc])\n",
    "    for _doc in child_doc:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    child_docs.extend(child_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 parent_docs의 개수: 4\n",
      "분할된 child_docs의 개수: 54\n"
     ]
    }
   ],
   "source": [
    "print(f\"분할된 parent_docs의 개수: {len(parent_docs)}\")\n",
    "print(f\"분할된 child_docs의 개수: {len(child_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(parent_docs)\n",
    "retriever.vectorstore.add_documents(child_docs)\n",
    "\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'doc_id': 'b270f99b-9b5e-4f0d-932d-408c12fcac58', 'source': './data/ai-story.txt'}),\n",
       " Document(page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'doc_id': 'be9ed566-69ff-469f-8c86-7656ec2692a7', 'source': './data/ai-story.txt'}),\n",
       " Document(page_content=', SARIMA는 계절적 패턴을 고려해야 하는 경우에 사용됩니다. SARIMAX는 외부 요인을 모델에 포함시켜야 할 때 가장 적합한 선택입니다.\\n\\nWord2Vec\\n\\nWord2Vec은 자연어 처리(NLP) 분야에서 널리 사용되는 획기적인 단어 임베딩 기법 중 하나입니다. 2013년 Google의 연구팀에 의해 개발되었으며, 단어를 벡터 공간에 매핑함으로써 컴퓨터가 단어의 의미를 수치적으로 이해할 수 있게 합니다. Word2Vec의 핵심 아이디어는 비슷한 맥락에서 사용되는 단어들은 비슷한 벡터를 가진다는 것입니다. 이를 통해 단어 간의 의미적 유사성과 관계를 벡터 연산을 통해 표현할 수 있습니다.', metadata={'doc_id': 'b270f99b-9b5e-4f0d-932d-408c12fcac58', 'source': './data/ai-story.txt'}),\n",
       " Document(page_content=', SARIMA는 계절적 패턴을 고려해야 하는 경우에 사용됩니다. SARIMAX는 외부 요인을 모델에 포함시켜야 할 때 가장 적합한 선택입니다.\\n\\nWord2Vec\\n\\nWord2Vec은 자연어 처리(NLP) 분야에서 널리 사용되는 획기적인 단어 임베딩 기법 중 하나입니다. 2013년 Google의 연구팀에 의해 개발되었으며, 단어를 벡터 공간에 매핑함으로써 컴퓨터가 단어의 의미를 수치적으로 이해할 수 있게 합니다. Word2Vec의 핵심 아이디어는 비슷한 맥락에서 사용되는 단어들은 비슷한 벡터를 가진다는 것입니다. 이를 통해 단어 간의 의미적 유사성과 관계를 벡터 연산을 통해 표현할 수 있습니다.', metadata={'doc_id': 'be9ed566-69ff-469f-8c86-7656ec2692a7', 'source': './data/ai-story.txt'})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"Word2Vec의 정의\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'doc_id': 'b270f99b-9b5e-4f0d-932d-408c12fcac58', 'source': './data/ai-story.txt'}),\n",
       "  0.9045879327309017),\n",
       " (Document(page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'doc_id': 'be9ed566-69ff-469f-8c86-7656ec2692a7', 'source': './data/ai-story.txt'}),\n",
       "  0.9045879327309017),\n",
       " (Document(page_content=', SARIMA는 계절적 패턴을 고려해야 하는 경우에 사용됩니다. SARIMAX는 외부 요인을 모델에 포함시켜야 할 때 가장 적합한 선택입니다.\\n\\nWord2Vec\\n\\nWord2Vec은 자연어 처리(NLP) 분야에서 널리 사용되는 획기적인 단어 임베딩 기법 중 하나입니다. 2013년 Google의 연구팀에 의해 개발되었으며, 단어를 벡터 공간에 매핑함으로써 컴퓨터가 단어의 의미를 수치적으로 이해할 수 있게 합니다. Word2Vec의 핵심 아이디어는 비슷한 맥락에서 사용되는 단어들은 비슷한 벡터를 가진다는 것입니다. 이를 통해 단어 간의 의미적 유사성과 관계를 벡터 연산을 통해 표현할 수 있습니다.', metadata={'doc_id': 'be9ed566-69ff-469f-8c86-7656ec2692a7', 'source': './data/ai-story.txt'}),\n",
       "  0.8445455428789942)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search_with_relevance_scores(\"Word2Vec의 정의\", k=3, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5733\n"
     ]
    }
   ],
   "source": [
    "relevant_doc = retriever.get_relevant_documents(\"Word2Vec의 정의?\")\n",
    "print(len(relevant_doc))\n",
    "print(len(relevant_doc[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import SearchType\n",
    "\n",
    "retriever.search_type = SearchType.mmr\n",
    "\n",
    "print(len(retriever.get_relevant_documents(\"Word2Vec의 정의\")))\n",
    "# len(retriever.get_relevant_documents(\"Word2Vec의 정의\")[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약본을 벡터DB에 같이 저장\n",
    "- 종종 청크보다 더 정확하게 정보를 추출할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/dudaji/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=os.environ[\"MODEL_ID\"], \n",
    "    max_new_tokens=2048,\n",
    "    temperature=0.1,\n",
    "    huggingfacehub_api_token=os.environ[\"HF_API_KEY\"],\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\n",
    "        \"Summarize the following document in Korean:\\n\\n{doc}\"\n",
    "    )\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a summary of the document in Korean:\\n\\n**Scikit-Learn**\\n\\nScikit-Learn은 Python 언어를 위한 기계 학습 라이브러리입니다. 2007년에 시작된 프로젝트로, 다양한 알고리즘을 구현하고 있습니다. Scikit-Learn은 분류, 회귀, 군집화, 차원 축소 등 다양한 기계 학습 작업을 지원하며, 사용하기 쉬운 API를 제공합니다. 라이브러리의 강점은 다양한 알고리즘을 일관된 인터페이스로 제공하는 것입니다.\\n\\n**NLP**\\n\\nNLP는 자연어 처리의 분야입니다. 컴퓨터 알고리즘과 기술을 통해 인간의 언어를 이해하고 해석합니다. NLP의 주요 응용 분야로는 기계 번역, 감정 분석, 음성 인식, 자동 요약, 챗봇 개발 등이 있습니다. NLP의 핵심 과제는 자연어의 모호성과 다양성을 처리하는 것입니다.\\n\\n**SciPy**\\n\\nSciPy는 과학 계산을 위한 파이썬 라이브러리입니다. 2001년에 시작된 프로젝트로, 고급 수학 함수, 알고리즘, 그리고 편리한 도구를 제공합니다. SciPy는 NumPy 배열 객체 위에 구축되어 있으며, NumPy의 기능을 확장하여 다양한 과학 및 공학 분야에서 필요로 하는 특정한 기능과 유틸리티를 제공합니다.\\n\\n**Hugging Face**\\n\\nHugging Face는 자연어 처리(NLP) 분야에서 선도적인 역할을 하는 기술 회사입니다. 2016년에 설립된 이 회사는 특히 \\'Transformers\\' 라이브러리를 통해 큰 인기를 얻었습니다. Transformers 라이브러리는 BERT, GPT, RoBERTa, T5와 같은 최신의 변환기(Transformer) 기반 모델을 쉽게 사용할 수 있게 해주며, Python 프로그래밍 언어로 작성되어 있습니다.\\n\\n**Attention is all you need**\\n\\n\"Attention Is All You Need\"는 2017년에 발표된 논문으로, 변환기(Transformer) 모델의 아키텍처를 처음으로 소개한 연구입니다. 이 논문은 자연어 처리(NLP)와 기계 학습 분야에 큰 영향을 미쳤습니다.\\n\\n**ARIMA, SARIMA, SARIMAX**\\n\\nARIMA, SARIMA, SARIMAX는 시계열 데이터 분석과 예측을 위한 통계 모델입니다. 각 모델은 특정 유형의 시계열 데이터에 대한 분석과 예측에 있어 강점을 가지며, 선택하는 모델은 분석하고자 하는 데이터의 특성과 요구사항에 따라 달라집니다.\\n\\n**Word2Vec**\\n\\nWord2Vec은 자연어 처리(NLP) 분야에서 널리 사용되는 획기적인 단어 임베딩 기법 중 하나입니다. 2013년 Google의 연구팀에 의해 개발되었으며, 단어를 벡터 공간에 매핑함으로써 컴퓨터가 단어의 의미를 수치적으로 이해할 수 있게 합니다. Word2Vec의 벡터 표현은 다양한 NLP 작업에 활용될 수 있습니다.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a summary of the document in Korean:\\n\\n**Semantic Search**\\n\\n* 의미론적 검색은 사용자의 질의를 분석하여 관련된 결과를 반환하는 검색 방식입니다.\\n* 예시: \"태양계 행성\" 검색하면 \"목성\", \"화성\" 등과 같은 관련된 행성 정보를 반환합니다.\\n\\n**Embedding**\\n\\n* 임베딩은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다.\\n* 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n\\n**Token**\\n\\n* 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다.\\n* 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n\\n**Vectorizer**\\n\\n* 벡터라이저는 텍스트 데이터를 벡터 형식으로 변환하는 도구입니다.\\n* 예시: \"I love programming.\"라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.\\n\\n**VectorStore**\\n\\n* 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다.\\n* 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\\n\\n**SQL**\\n\\n* SQL은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다.\\n* 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\\n\\n**CSV**\\n\\n* CSV는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다.\\n* 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n\\n**JSON**\\n\\n* JSON은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n* 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n\\n**Transformer**\\n\\n* 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다.\\n* 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n\\n**HuggingFace**\\n\\n* HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다.\\n* 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\\n\\n**Digital Transformation**\\n\\n* 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다.\\n* 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\\n\\n**Crawling**\\n\\n* 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다.\\n* 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n\\n**Word2Vec**\\n\\n* Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다.\\n* 예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n\\n**LLM (Large Language Model)**\\n\\n* LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다.\\n* 예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n\\n**FAISS (Facebook AI Similarity Search)**\\n\\n* FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\\n* 예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n\\n**Open Source**\\n\\n* 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다.\\n* 예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n\\n**Structured Data**\\n\\n* 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다.\\n* 예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n\\n**Parser**\\n\\n* 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다.\\n* 예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n\\n**TF-IDF (Term Frequency-Inverse Document Frequency)**\\n\\n* TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다.\\n* 예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n\\n**Deep Learning**\\n\\n* 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다.\\n* 예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n\\n**Schema**\\n\\n* 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n* 예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n\\n**DataFrame**\\n\\n* DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.\\n* 예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.\\n\\n**Attention 메커니즘**\\n\\n* Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 \\'주의\\'를 기울이도록 하는 기법입니다.\\n* 예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\\n\\n**Pandas**\\n\\n* 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다.\\n* 예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n\\n**GPT (Generative Pretrained Transformer)**\\n\\n* GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다.\\n* 예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n\\n**InstructGPT**\\n\\n* InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다.\\n* 예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.\\n\\n**Keyword Search**\\n\\n* 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다.\\n* 예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\\n\\n**Page Rank**\\n\\n* 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다.\\n* 예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\\n\\n**Data Mining**\\n\\n* 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다.\\n* 예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\\n\\n**Multimodal**\\n\\n* 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다.\\n* 예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dudaji/anaconda3/envs/llm-study/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 요약 정보를 저장할 벡터 저장소를 생성합니다.\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=HuggingFaceBgeEmbeddings())\n",
    "# 부모 문서를 저장할 저장소를 생성합니다.\n",
    "store = InMemoryByteStore()\n",
    "# 문서 ID를 저장할 키 이름을 지정합니다.\n",
    "id_key = \"doc_id\"\n",
    "# 검색기를 초기화합니다. (시작 시 비어 있음)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,  # 벡터 저장소\n",
    "    byte_store=store,  # 바이트 저장소\n",
    "    id_key=id_key,  # 문서 ID 키\n",
    ")\n",
    "# 문서 ID를 생성합니다.\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    # 요약된 내용을 페이지 콘텐츠로 하고, 문서 ID를 메타데이터로 포함하는 Document 객체를 생성합니다.\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(\n",
    "        summaries\n",
    "    )  # summaries 리스트의 각 요약과 인덱스에 대해 반복합니다.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(\n",
    "    summary_docs\n",
    ")  # 요약된 문서를 벡터 저장소에 추가합니다.\n",
    "\n",
    "# 문서 ID와 문서를 매핑하여 문서 저장소에 저장합니다.\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RecursiveCharacterTextSplitter 객체를 생성합니다.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "\n",
    "split_docs = []\n",
    "split_docs_ids = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]  # 현재 문서의 ID를 가져옵니다.\n",
    "    # 현재 문서를 하위 문서로 분할합니다.\n",
    "    split_doc = text_splitter.split_documents([doc])\n",
    "    for _doc in split_doc:  # 분할된 문서에 대해 반복합니다.\n",
    "        # 문서의 메타데이터에 ID를 저장합니다.\n",
    "        _doc.metadata[id_key] = _id\n",
    "        split_docs_ids.append(_id)\n",
    "    split_docs.extend(split_doc)  # 분할된 문서를 리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['749f750c-7cbf-4284-8c22-96ec88102201',\n",
       " '23c75217-2921-4653-852b-8a07a6c596ac',\n",
       " 'dec45d63-97b3-4e7c-9196-c0505a50173d',\n",
       " '8ef458b7-3373-4379-9c9c-a61d8e5a5f8e',\n",
       " 'be45a6ea-b564-4b92-8f0f-fec2a18e3140',\n",
       " '82a59b52-b83d-47a3-b1e7-8ddbb0c61fa4',\n",
       " '457440b0-30a7-4384-bace-0036b6a6fe57',\n",
       " 'b2865e63-63c5-433b-9c83-f96a0e8e28d2',\n",
       " 'e140b78a-042e-4d38-be2e-6115cf9ea54c',\n",
       " '8cced656-02f5-4527-8ccc-6bb642380092',\n",
       " '49b539fd-1b4c-43d2-89e3-9a899a17ac07',\n",
       " 'ace96879-dc79-493e-b00f-b8b7903fce3e',\n",
       " '5374a86b-085f-4727-b6ca-9fda39b280c1',\n",
       " 'daa05061-7ef7-4d10-b31a-f5967f1e58a6',\n",
       " '2eb6caf9-3100-4714-8023-a81df65617a7',\n",
       " '2a0dcdb1-134b-47e3-b76d-8c03eb9b97fa',\n",
       " '4e85311e-bef5-4c41-953d-b315adfe9381']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.add_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Word2Vec의 성공 이후, 이와 유사한 다른 단어 임베딩 기법들도 개발되었습니다. 그러나 Word2Vec은 그 간결함과 효율성, 높은 성능으로 인해 여전히 광범위하게 사용되며, NLP 분야에서 기본적인 도구로 자리 잡았습니다. Word2Vec는 단순한 텍스트 데이터를 통해 복잡한 언어의 의미 구조를 학습할 수 있는 강력한 방법을 제공함으로써, 컴퓨터가 인간 언어를 이해하는 방식을 혁신적으로 개선하였습니다.', metadata={'doc_id': 'd1baca23-2cb7-4cb9-91f1-d804ab0a866b', 'source': './data/ai-story.txt'})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_docs = vectorstore.similarity_search(\"Word2Vec의 정의가 뭐야?\")\n",
    "result_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\"Word2Vec 의 정의가 뭐야?\")\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7482"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가설 쿼리\n",
    "- LLM은 문서에 대해 가정할 수 있는 질문 목록을 만들 수 있다\n",
    "- 이를 이용하여 가설 질문을 임베딩하여 문서 내용을 더 깊이있게 탐색할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"hypothetical_questions\",  # 함수의 이름을 지정합니다.\n",
    "        \"description\": \"Generate hypothetical questions\",  # 함수에 대한 설명을 작성합니다.\n",
    "        \"parameters\": {  # 함수의 매개변수를 정의합니다.\n",
    "            \"type\": \"object\",  # 매개변수의 타입을 객체로 지정합니다.\n",
    "            \"properties\": {  # 객체의 속성을 정의합니다.\n",
    "                \"questions\": {  # 'questions' 속성을 정의합니다.\n",
    "                    \"type\": \"array\",  # 'questions'의 타입을 배열로 지정합니다.\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },  # 배열의 요소 타입을 문자열로 지정합니다.\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"questions\"],  # 필수 매개변수로 'questions'를 지정합니다.\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer.\\n\\n{doc}\\n{format_instructions}\",\n",
    "        input_variables=[\"doc\"],\n",
    "        partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    "    )\n",
    ")\n",
    "prompt_chain = {\"doc\": lambda x: x.page_content} | ChatPromptTemplate.from_messages([human_message])\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    # 아래 문서를 사용하여 답변할 수 있는 가상의 질문을 정확히 3개 생성하도록 요청합니다. 이 숫자는 조정될 수 있습니다.\n",
    "    | ChatPromptTemplate.from_messages([human_message])\n",
    "    | model\n",
    "    # 출력에서 \"questions\" 키에 해당하는 값을 추출합니다.\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three hypothetical questions that the provided document could be used to answer:\\n\\nScikit-Learn',\n",
       " 'NLP',\n",
       " 'SciPy',\n",
       " 'HuggingFace',\n",
       " 'Attention is all you need',\n",
       " 'ARIMA',\n",
       " 'SARIMA',\n",
       " 'SARIMAX',\n",
       " \"Word2Vec\\n\\nHere are the questions:\\n\\n1. What are the key features of Scikit-Learn and how does it differ from other machine learning libraries?\\n2. How does HuggingFace's Transformers library contribute to the development of natural language processing (NLP) and what are its applications?\\n3. What are the differences between ARIMA\",\n",
       " 'SARIMA',\n",
       " 'and SARIMAX models in time series analysis',\n",
       " 'and when would you use each one?']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_chain.invoke(docs[0])\n",
    "chain.invoke(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
