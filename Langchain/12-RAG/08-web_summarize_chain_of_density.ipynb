{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import textwrap\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load some data to summarize\n",
    "loader = WebBaseLoader(\"https://teddylee777.github.io/data-science/optuna/\")\n",
    "docs = loader.load()\n",
    "content = docs[0].page_content\n",
    "\n",
    "# Get this prompt template\n",
    "prompt = hub.pull(\"lawwu/chain_of_density\")\n",
    "# The chat model output is a JSON list of dicts, with SimpleJsonOutputParser\n",
    "# we can convert it o a dict, and it suppors streaming.\n",
    "json_parser = SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Article: {ARTICLE}\n",
    "You will generate increasingly concise, entity-dense summaries of the above article. \n",
    "\n",
    "Repeat the following 2 steps 5 times. \n",
    "\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main story, \n",
    "- specific yet concise (50 words or fewer), \n",
    "- novel (not in the previous summary), \n",
    "- faithful (present in the article), \n",
    "- anywhere (can be located anywhere in the article).\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "\"\"\"\n",
    ")\n",
    "# Use only KOREAN language to reply.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=os.environ[\"MODEL_ID\"], \n",
    "#     max_new_tokens=2048,\n",
    "#     temperature=0.1,\n",
    "#     huggingfacehub_api_token=os.environ[\"HF_API_KEY\"],\n",
    "# )\n",
    "# model = ChatHuggingFace(llm=llm)\n",
    "model = ChatOllama(model=\"gemma:7b\")\n",
    "# llm = ChatOllama(model=\"mistral:7b\")\n",
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"beomi/Yi-Ko-6B\",\n",
    "#     model_kwargs={\"temperature\": 0.1, \"max_length\": 2048},\n",
    "#     task=\"text-generation\",\n",
    "# )\n",
    "# model = ChatHuggingFace(llm=llm)\n",
    "# model = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"beomi/Yi-Ko-6B\",\n",
    "#     model_kwargs={\"temperature\": 0.1, \"max_length\": 1000000},\n",
    "#     task=\"text-generation\",\n",
    "#     device=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"ARTICLE\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | json_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Missing_Entities': 'AI assistant, search engine optimization, natural language processing',\n",
       "  'Denser_Summary': 'This article explores the application of AI assistants in search engine optimization. It highlights the importance of natural language processing for effective interaction and the potential for AI-powered assistants to optimize website rankings and user engagement.'},\n",
       " {'Missing_Entities': 'data analysis, visualization, sentiment analysis',\n",
       "  'Denser_Summary': 'The article emphasizes the role of data analysis in understanding user behavior and optimizing website content. It suggests the use of visualization and sentiment analysis techniques to gain insights from data and refine the search engine optimization strategy.'},\n",
       " {'Missing_Entities': 'competitive analysis, keyword research, backlink analysis',\n",
       "  'Denser_Summary': 'The article discusses the importance of competitive analysis to identify industry trends and identify relevant keywords. It also highlights the significance of backlink analysis for improving website authority.'},\n",
       " {'Missing_Entities': 'user experience design, mobile optimization, accessibility',\n",
       "  'Denser_Summary': 'The article emphasizes the need for a seamless user experience, highlighting the importance of mobile optimization and accessibility in search engine optimization. It suggests,\\nmore\\n...\\n...\\n'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
